{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8079c154-8163-4491-a9bb-080e79c07810",
   "metadata": {},
   "source": [
    "Universidad del Valle de Guatemala <br>\n",
    "Facultad de Ingeniería <br>\n",
    "Departamento de Ciencias de la computación <br>\n",
    "Inteligencia Artificial \n",
    "\n",
    "# Laboratorio 10\n",
    "\n",
    "## Integrantes\n",
    "Christopher García <br>\n",
    "Ma. Isabel Solano <br>\n",
    "Ale Gómez <br>\n",
    "Roby Vallecillos <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15f2f37d-635c-4c55-8e59-5364bec71ca6",
   "metadata": {},
   "source": [
    "## Task 1 - Clasificador de Imágenes (Perros vs Gatos)\n",
    "Deberá construir un modelo basado en redes neuronales que le permita clasificar perros y gatos basado en un input de imágen. Para esto, deberá usar técnicas basadas en Deep Learning relacionadas con el manejo de imágenes como las redes  convolucionales. Esto lo deberá hacer usando su librería de preferencia como PyTorch, Tensoflow\n",
    "entre otras. Recuerden que: \n",
    "- Deben hacer una breve exploración con los datos. Esto implica, pero no está limitado a:\n",
    "    - Limpieza de imágenes\n",
    "    - Escalamiento de las imágenes\n",
    "    - Estandarización de color\n",
    "    - Manipulación de imágenes\n",
    "        - Agregar contorno\n",
    "        - Cambiar dirección\n",
    "    - Revisar si el dataset está balanceado, caso no estarlo, aplicar alguna técnica para balancearlo lo más y mejor posible\n",
    "- Recuerden hacer el split para training, testing y si consideran necesario para validation\n",
    "    - 80% training\n",
    "    - 20% testing\n",
    "        - 10% validation si lo necesitan\n",
    "- Recuerde definir de forma clara y razonada (es decir, diga el por qué de su elección) de una métrica de desempeño principal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "161c61c5-8024-451a-a176-c310280a40eb",
   "metadata": {},
   "source": [
    "### Task 1.1 - Lectura del Dataset\n",
    "En esta ocasión no recibirá un documento único como entrada de datos, por el contrario deberá recibir un conjunto de imágenes que representan perros y gatos. Estos datos los puede descargar de este dataset de Kaggle. Deberá leer estas imágenes para crear un dataset en memoria que haga referencia a la imagen con sus características y a la categoría a la que pertenece. Recuerde que debe dividir su dataset según el uso y como se mencionó anteriormente. Además, considere que la cantidad de imágenes de perros y gatos puede estar desigual por lo que deberá aplicar las técnicas aprendidas durante el curso para lidiar con este problema Son libres de usar todas las imágenes del dataset dado o bien una cantidad definida por ustedes mismos siempre y cuando se argumente la razón de forma debida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496f5304-1b7a-4bce-916d-32e6f5530433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bed5482-c7dd-4917-8426-aa56844a4202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number de categorías: 2\n",
      "Cantidad de perros:  12501\n",
      "Cantidad de gatos:  12500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"PetImages\"  # Replace this with the actual path to the directory\n",
    "num_elements = len(os.listdir(path))\n",
    "\n",
    "print(\"Number de categorías:\", num_elements)\n",
    "print(\"Cantidad de perros: \", len(os.listdir(\"PetImages/Dog\")))\n",
    "print(\"Cantidad de gatos: \", len(os.listdir(\"PetImages/Cat\")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13aef091-bf26-48ca-958e-ca70f55d1c08",
   "metadata": {},
   "source": [
    "### Task 1.2 - Construcción del Modelo\n",
    "Deberá crear un modelo de Deep Learning orientado a imágenes, como una red que usa capas convolucionales, para poder resolver este laboratorio. Recuerde que deberá definir la arquitectura de su red, aplicando las diferentes técnicas vistas en clase (dropouts, funciones de activación, padding, stride, etc). Podrá usar la librería que más le parezca para completar el laboratorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8934caf0-6966-4503-9936-7eaad81e7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiendo transformers\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c39e450-3167-42a1-86e3-41194319cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"PetImages\"  # Replace this with the actual path to the dataset\n",
    "image_datasets = datasets.ImageFolder(data_dir, transform=data_transforms)\n",
    "dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c4b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(image_datasets))\n",
    "test_size = len(image_datasets) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(image_datasets, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd81278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54db8d17-7183-4d87-be22-98d8f803ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\"\"\"\n",
    "Referencias:\n",
    "ResNet implementation in PyTorch: https://pytorch.org/vision/stable/models.html#torchvision.models.resnet18\n",
    "torch.nn.Conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "torch.nn.BatchNorm2d: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "torch.nn.ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "torch.nn.MaxPool2d: https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "torch.nn.AdaptiveAvgPool2d: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n",
    "torch.nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\"\"\"\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = ResNet18(num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4b3a72-decf-41fe-adda-a02cc215eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de función de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c3220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6717, Accuracy: 0.5930\n",
      "Epoch [2/10], Loss: 0.6263, Accuracy: 0.6479\n",
      "Epoch [3/10], Loss: 0.5979, Accuracy: 0.6647\n",
      "Epoch [4/10], Loss: 0.5649, Accuracy: 0.7095\n",
      "Epoch [5/10], Loss: 0.5433, Accuracy: 0.7363\n",
      "Epoch [6/10], Loss: 0.5216, Accuracy: 0.7491\n",
      "Epoch [7/10], Loss: 0.4895, Accuracy: 0.7795\n",
      "Epoch [8/10], Loss: 0.4219, Accuracy: 0.8139\n",
      "Epoch [9/10], Loss: 0.4067, Accuracy: 0.8139\n",
      "Epoch [10/10], Loss: 0.3546, Accuracy: 0.8543\n"
     ]
    }
   ],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_len = int(len(image_datasets) * 0.8)\n",
    "test_len = len(image_datasets) - train_len\n",
    "\n",
    "train_dataset, test_dataset = random_split(image_datasets, [train_len, test_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "        except UnidentifiedImageError:\n",
    "            continue\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1932acc2-3ce2-4170-b6f2-5bef93091451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.739\n",
      "[1,   400] loss: 0.693\n",
      "[1,   600] loss: 0.694\n",
      "[1,   800] loss: 0.693\n",
      "[1,  1000] loss: 0.694\n",
      "[1,  1200] loss: 0.691\n",
      "[1,  1400] loss: 0.694\n",
      "[1,  1600] loss: 0.690\n",
      "[1,  1800] loss: 0.689\n",
      "[1,  2000] loss: 0.690\n",
      "[1,  2200] loss: 0.692\n",
      "[1,  2400] loss: 0.681\n",
      "[1,  2600] loss: 0.682\n",
      "[1,  2800] loss: 0.688\n",
      "[1,  3000] loss: 0.672\n",
      "[1,  3200] loss: 0.691\n",
      "[1,  3400] loss: 0.693\n",
      "[1,  3600] loss: 0.690\n",
      "[1,  3800] loss: 0.689\n",
      "[1,  4000] loss: 0.689\n",
      "[1,  4200] loss: 0.682\n",
      "[1,  4400] loss: 0.692\n",
      "[1,  4600] loss: 0.676\n",
      "[1,  4800] loss: 0.685\n",
      "[1,  5000] loss: 0.679\n",
      "[1,  5200] loss: 0.666\n",
      "[1,  5400] loss: 0.671\n",
      "[1,  5600] loss: 0.673\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BufferedReader name='PetImages/Dog/11702.jpg'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/Users/alegomez/Documents/UVG/Cuarto Año/Primer Semestre/IA/UVG_IA_Lab10/Lab10.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alegomez/Documents/UVG/Cuarto%20A%C3%B1o/Primer%20Semestre/IA/UVG_IA_Lab10/Lab10.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alegomez/Documents/UVG/Cuarto%20A%C3%B1o/Primer%20Semestre/IA/UVG_IA_Lab10/Lab10.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alegomez/Documents/UVG/Cuarto%20A%C3%B1o/Primer%20Semestre/IA/UVG_IA_Lab10/Lab10.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders, \u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alegomez/Documents/UVG/Cuarto%20A%C3%B1o/Primer%20Semestre/IA/UVG_IA_Lab10/Lab10.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         inputs, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alegomez/Documents/UVG/Cuarto%20A%C3%B1o/Primer%20Semestre/IA/UVG_IA_Lab10/Lab10.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[1;32m    245\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 247\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(f)\n\u001b[1;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:3123\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3121\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m accept_warnings:\n\u001b[1;32m   3122\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message)\n\u001b[0;32m-> 3123\u001b[0m \u001b[39mraise\u001b[39;00m UnidentifiedImageError(\n\u001b[1;32m   3124\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcannot identify image file \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (filename \u001b[39mif\u001b[39;00m filename \u001b[39melse\u001b[39;00m fp)\n\u001b[1;32m   3125\u001b[0m )\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BufferedReader name='PetImages/Dog/11702.jpg'>"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloaders, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1f985c9-3aef-4b91-bf96-96858d61de12",
   "metadata": {},
   "source": [
    "### Task 1.3 - Desempeño del modelo\n",
    "Al finalizar el entrenamiento de su modelo, permita que se ingresen nuevas imágenes y permita que el modelo la clasifique. Además recuerde medir el desempeño de su modelo tanto en entrenamiento como en testing. Para ello, deberá mostrar las métricas de desempeño de su modelo para las fases dichas, además de la evolución de las métricas durante las diferentes épocas, cuidando siempre no hacer overfitting sobre el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279ab87d-9bd9-4411-a94f-9650f222e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, UnidentifiedImageError\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "try:\n",
    "    image_path = input(\"Enter the path to an image: \")\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        print(\"Prediction:\", predicted.item())\n",
    "except (UnidentifiedImageError, FileNotFoundError):\n",
    "    print(\"ERROR\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9596dde0",
   "metadata": {},
   "source": [
    "### Resultados:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f36f5b1",
   "metadata": {},
   "source": [
    "Se etiquetó correctamente el archivo, siendo este el de Cosmo, el perro de Guardianes de la Galaxia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6312169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Valores obtenidos en inciso anterior:\n",
    "accuracies = [0.5930, 0.6479, 0.6647, 0.7095, 0.7363, 0.7491, 0.7795, 0.8139, 0.8139, 0.8543]\n",
    "\n",
    "# Gráfico\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(accuracies) + 1), accuracies, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Graph')\n",
    "plt.xticks(range(1, len(accuracies) + 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3efce2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograma\n",
    "import seaborn as sns\n",
    "sns.histplot(accuracies, bins=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
